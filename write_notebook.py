import json
import os

notebook_content = {
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": None,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve, auc, confusion_matrix, roc_curve\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Determine the project root directory reliably\n",
    "# This assumes the notebook is in 'credit-risk-model/notebooks/'\n",
    "notebook_current_dir = os.path.dirname(os.path.abspath(''))\n",
    "project_root_dir = os.path.abspath(os.path.join(notebook_current_dir, '..'))\n",
    "\n",
    "# Add the project root to sys.path so modules in 'src' can be imported\n",
    "if project_root_dir not in sys.path:\n",
    "    sys.path.append(project_root_dir)\n",
    "\n",
    "print(f\"Project Root Directory: {project_root_dir}\")\n",
    "print(f\"sys.path after adding project root: {sys.path}\")\n",
    "\n",
    "# Now import from src.data_processing and src.model_training\n",
    "from src.data_processing import get_data_processing_pipeline, generate_proxy_target\n",
    "from src.model_training import train_and_log_model\n",
    "\n",
    "print(\"Setup complete for Model Evaluation and Interpretation notebook.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": None,
   "outputs": [],
   "source": [
    "print(\"Loading and processing data...\")\n",
    "\n",
    "# Determine the project root directory reliably (same logic as Cell 1)\n",
    "notebook_current_dir = os.path.dirname(os.path.abspath(''))\n",
    "project_root_dir = os.path.abspath(os.path.join(notebook_current_dir, '..'))\n",
    "\n",
    "# Path to data.csv: relative to the project root\n",
    "raw_data_file_path = os.path.join(project_root_dir, 'data', 'raw', 'data.csv')\n",
    "\n",
    "raw_df = None # Initialize raw_df to None\n",
    "try:\n",
    "    raw_df = pd.read_csv(raw_data_file_path)\n",
    "    print(f\"Raw data loaded successfully from: {raw_data_file_path}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Data file not found at {raw_data_file_path}. Please ensure it exists.\")\n",
    "    raise # Re-raise the error to stop execution clearly\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred while loading data: {e}\")\n",
    "    raise # Re-raise other exceptions\n",
    "\n",
    "if raw_df is None: # Defensive check, though re-raising should prevent this\n",
    "    raise ValueError(\"raw_df could not be loaded. Aborting data processing.\")\n",
    "\n",
    "pipeline = get_data_processing_pipeline()\n",
    "processed_data_array = pipeline.fit_transform(raw_df.copy())\n",
    "\n",
    "preprocessor_step = pipeline.named_steps['preprocessor']\n",
    "num_features_out = preprocessor_step.named_transformers_['num'].get_feature_names_out()\n",
    "cat_features_out = preprocessor_step.named_transformers_['cat'].get_feature_names_out()\n",
    "\n",
    "df_before_preprocessor = pipeline.named_steps['datetime_extractor'].transform(raw_df.copy())\n",
    "df_before_preprocessor = pipeline.named_steps['amount_handler'].transform(df_before_preprocessor)\n",
    "processed_by_preprocessor = ['Amount', 'Value', 'PricingStrategy',\n",
    "                             'ProductCategory', 'ChannelId', 'ProviderId', 'ProductId']\n",
    "passthrough_cols_final = [col for col in df_before_preprocessor.columns if col not in processed_by_preprocessor]\n",
    "final_feature_names = list(num_features_out) + list(cat_features_out) + passthrough_cols_final\n",
    "\n",
    "processed_df = pd.DataFrame(processed_data_array, columns=final_feature_names)\n",
    "\n",
    "numeric_cols_to_convert = list(num_features_out) + \\\n",
    "                          list(cat_features_out) + \\\n",
    "                          ['transaction_hour', 'transaction_day_of_week', 'transaction_month',\n",
    "                           'transaction_year', 'is_refund', 'FraudResult']\n",
    "\n",
    "for col in numeric_cols_to_convert:\n",
    "    if col in processed_df.columns:\n",
    "        processed_df[col] = pd.to_numeric(processed_df[col], errors='coerce')\n",
    "\n",
    "proxy_target_df = generate_proxy_target(raw_df.copy())\n",
    "proxy_target_df_reset = proxy_target_df.reset_index()\n",
    "\n",
    "final_processed_df = pd.merge(\n",
    "    processed_df,\n",
    "    proxy_target_df_reset[['CustomerId', 'is_high_risk']],\n",
    "    on='CustomerId',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "X = final_processed_df.drop(columns=[\n",
    "    'TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId',\n",
    "    'CurrencyCode', 'CountryCode', 'TransactionStartTime', 'FraudResult',\n",
    "    'is_high_risk'\n",
    "])\n",
    "y = final_processed_df['is_high_risk']\n",
    "\n",
    "initial_rows = X.shape[0]\n",
    "X = X.dropna(axis=1)\n",
    "X = X.dropna(axis=0)\n",
    "rows_after_na = X.shape[0]\n",
    "if initial_rows != rows_after_na:\n",
    "    print(f\"Dropped {initial_rows - rows_after_na} rows due to NaN values after processing.\")\n",
    "\n",
    "y = y.loc[X.index]\n",
    "\n",
    "print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "print(f\"Target distribution (is_high_risk):\\n{y.value_counts(normalize=True)}\")\n",
    "\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    ")\n",
    "\n",
    "print(\"Data loading and processing complete. Data split into training and testing sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": None,
   "outputs": [],
   "source": [
    "print(\"Loading the latest trained model from MLflow...\")\n",
    "\n",
    "# Determine the project root directory reliably (same logic as Cell 1)\n",
    "notebook_current_dir = os.path.dirname(os.path.abspath(''))\n",
    "project_root_dir = os.path.abspath(os.path.join(notebook_current_dir, '..'))\n",
    "\n",
    "# MLflow tracking URI: 'mlruns' is a subdirectory within 'notebooks/' (relative to project root)\n",
    "mlflow_tracking_uri = os.path.join(project_root_dir, 'notebooks', 'mlruns')\n",
    "mlflow.set_tracking_uri(f\"file://{mlflow_tracking_uri}\")\n",
    "\n",
    "client = mlflow.tracking.MlflowClient()\n",
    "experiment_name = \"Credit_Risk_Model_Training\"\n",
    "\n",
    "try:\n",
    "    experiment = client.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        raise ValueError(f\"Experiment '{experiment_name}' not found.\")\n",
    "\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        order_by=[\"start_time DESC\"],\n",
    "        max_results=1\n",
    "    )\n",
    "\n",
    "    if not runs:\n",
    "        raise ValueError(f\"No runs found for experiment '{experiment_name}'. Please train a model first.\")\n",
    "\n",
    "    latest_run = runs[0]\n",
    "    latest_run_id = latest_run.info.run_id\n",
    "    print(f\"Latest MLflow Run ID: {latest_run_id}\")\n",
    "\n",
    "    model_uri = f\"runs:/{latest_run_id}/logistic_regression_model\"\n",
    "    loaded_model = mlflow.sklearn.load_model(model_uri)\n",
    "    print(\"Model loaded successfully from MLflow.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from MLflow: {e}\")\n",
    "    print(\"Please ensure you have run the 3.0-model-training.ipynb notebook to log a model.\")\n",
    "    raise # Re-raise the exception to stop execution clearly\n",
    "\n",
    "print(\"Model loading complete.\")\n"
   ]
  }
 ]
}

notebook_file_path = os.path.join(os.getcwd(), 'notebooks', '4.0-model-evaluation-and-interpretation.ipynb')

try:
    with open(notebook_file_path, 'w') as f:
        json.dump(notebook_content, f, indent=2)
    print(f"Successfully wrote notebook content to: {notebook_file_path}")
except Exception as e:
    print(f"Error writing notebook file: {e}")
